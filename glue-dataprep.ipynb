{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation using AWS Glue Dev Endpoints and Sagemaker Notebook <a name=\"top\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this notebook, we will do the following activities:\n",
    "    \n",
    "- Build a Star (Denormalized) Schema from an OLTP 3NF (3rd Normal Form) Schema.\n",
    "- Write the derived table for denorm data set in parquet format partitioned out by key fields.\n",
    "- Finally, orchestrate the pipeline to create an AWS Glue Workflow.\n",
    "\n",
    "Let's start by connecting to our our AWS Glue Dev Endpoint - a persistent AWS Glue Spark  Development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:13:36.786585Z",
     "start_time": "2020-05-20T20:13:12.192109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1592850335670_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-5-132.ec2.internal:20888/proxy/application_1592850335670_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-229.ec2.internal:8042/node/containerlogs/container_1592850335670_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a04d2a04de47009138535257827787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137f3571e3f5475d994f03066126ef83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'2.4.3'"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:24.960531Z",
     "start_time": "2020-05-20T20:14:21.561942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b7fe77e26f4129904b163b77e631f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|    default2|\n",
      "|   nyc_trips|\n",
      "|     salesdb|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:26.609627Z",
     "start_time": "2020-05-20T20:14:25.773077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f4360296ff48f39abc32dd1083ccc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use salesdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:26.609627Z",
     "start_time": "2020-05-20T20:14:25.773077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66377cd16964040ad2ac1bbd29cb1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------+\n",
      "|database|         tableName|isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "| salesdb|          customer|      false|\n",
      "| salesdb|     customer_site|      false|\n",
      "| salesdb|           product|      false|\n",
      "| salesdb|  product_category|      false|\n",
      "| salesdb|       sales_order|      false|\n",
      "| salesdb|   sales_order_all|      false|\n",
      "| salesdb|sales_order_detail|      false|\n",
      "| salesdb|          supplier|      false|\n",
      "+--------+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that regular Spark SQL commands work great as we have enabled the feature 'Use Glue Data Catalog as the Hive metastore' for our AWS Glue Dev Endpoint by default. You can choose to run any spark-sql commands against these tables as an optional exercise \n",
    "\n",
    "You can click on the link to read more on [AWS Glue Data Catalog Support for Spark SQL Jobs](\n",
    "https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-data-catalog-hive.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation by denormalizing the tables and writing them in parquet format\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this activity, we will denormalize an OLTP 3NF schema to Parquet. This activity demonstrates the using AWS Glue operations to perform powerful data transformations on input data:\n",
    "\n",
    "![alt text](../resources/denormalize.png \"Building a Star Schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:28:10.219411Z",
     "start_time": "2020-05-20T20:28:09.349347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1bc40d7f0f4bf7b4006696b366378c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+------------------+-----------+\n",
      "|database|         tableName|isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "| salesdb|          customer|      false|\n",
      "| salesdb|     customer_site|      false|\n",
      "| salesdb|           product|      false|\n",
      "| salesdb|  product_category|      false|\n",
      "| salesdb|       sales_order|      false|\n",
      "| salesdb|   sales_order_all|      false|\n",
      "| salesdb|sales_order_detail|      false|\n",
      "| salesdb|          supplier|      false|\n",
      "+--------+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use salesdb\").show()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the dataset\n",
    "\n",
    "Let's now denormalize the source tables where applicable and write out the data in Parquet format to the destination location. Not to change the S3 output_path in the cell below to appropriate bucket in your account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:38.734325Z",
     "start_time": "2020-05-20T20:19:38.618397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33303f0743394838a0099944b50ba3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## We will simulate the Glue job arguments \n",
    "import sys\n",
    "sys.argv = [\"LabDataPrepJob\",\"--JOB_NAME\", \"LabDataPrepJob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the code for the AWS Glue Job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:42.008393Z",
     "start_time": "2020-05-20T20:19:41.180254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc201b520fdc441da95f1f9d4f689798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabDataPrepJob START..."
     ]
    }
   ],
   "source": [
    "## Glue boilerplate code\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import boto3, json\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "print (args['JOB_NAME']+\" START...\")\n",
    "if 'sc' not in vars(): sc = SparkContext()\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "## Glue boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:31:53.789869Z",
     "start_time": "2020-05-20T20:28:13.599887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16824bd08a54fd08ac4669bd59e0416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://glue-labs-001-518010202968/data/sales_analytics/customer_dim\n",
      "<awsglue.dynamicframe.DynamicFrame object at 0x7fbf9b111410>"
     ]
    }
   ],
   "source": [
    "db_name='salesdb'\n",
    "table1='customer'\n",
    "table2='customer_site'\n",
    "output_dir='s3://glue-labs-001-518010202968/data/sales_analytics/customer_dim'\n",
    "print (output_dir)\n",
    "\n",
    "# Read the Source Tables\n",
    "cust_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "cust_site_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table2)\n",
    "\n",
    "# Join the two Source Tables\n",
    "customer_dim_dyf = Join.apply(cust_dyf,cust_site_dyf,\n",
    "                       'cust_id', 'cust_id').drop_fields(['cust_id'])\n",
    "\n",
    "# Write the denormalized CUSTOMER_DIM table in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = customer_dim_dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir}, format = \"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:13.410229Z",
     "start_time": "2020-05-20T20:32:08.057009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8648462bd94ac994d6805845132337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://glue-labs-001-518010202968/data/sales_analytics/product_dim/\n",
      "<awsglue.dynamicframe.DynamicFrame object at 0x7fbf9b0b2150>"
     ]
    }
   ],
   "source": [
    "table1='product_category'\n",
    "table2='product'\n",
    "output_dir='s3://glue-labs-001-518010202968/data/sales_analytics/product_dim/'\n",
    "print (output_dir)\n",
    "\n",
    "# Read the Source Tables\n",
    "table1_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "table2_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table2)\n",
    "\n",
    "#Join the Source Tables\n",
    "product_dim_dyf = Join.apply(table1_dyf,table2_dyf,\n",
    "                       'category_id', 'category_id').drop_fields(['category_id'])\n",
    "\n",
    "# Write the denormalized CUSTOMER_DIM table in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = product_dim_dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir}, format = \"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:44.853461Z",
     "start_time": "2020-05-20T20:32:41.508965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be369b077a89450d9202daf713826025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://glue-labs-001-518010202968/data/sales_analytics/supplier_dim/\n",
      "<awsglue.dynamicframe.DynamicFrame object at 0x7fbf9b111790>"
     ]
    }
   ],
   "source": [
    "table1='supplier'\n",
    "output_dir='s3://glue-labs-001-518010202968/data/sales_analytics/supplier_dim/'\n",
    "print (output_dir)\n",
    "\n",
    "# Read the Source Tables\n",
    "table1_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "\n",
    "\n",
    "# Write the denormalized CUSTOMER_DIM table in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = table1_dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir}, format = \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:45.903127Z",
     "start_time": "2020-05-20T20:32:45.787803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9d91ae7c584c5ab35e5eb43f0d6505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://glue-labs-001-518010202968/data/sales_analytics/sales_order_fact/"
     ]
    }
   ],
   "source": [
    "table1='sales_order_detail'\n",
    "table2='sales_order'\n",
    "output_dir='s3://glue-labs-001-518010202968/data/sales_analytics/sales_order_fact/'\n",
    "print (output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'sales_order_fact' table, we will try a different approach - \n",
    "\n",
    "- We will convert the AWS Glue DynamicFrame to a Spark DataFrame\n",
    "- Register the Spark Dataframe to a Spark Temporary View\n",
    "- Use Spark SQL to build the write out the target dataset.\n",
    "\n",
    "This demonstrates that AWS Glue DynamicFrames and Spark Dataframes are interchangeable and you can get the best of both worlds by using both the options where suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:50.925429Z",
     "start_time": "2020-05-20T20:32:48.590150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04efacef9c54e5fb23c91b5b8315f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the Source Tables\n",
    "table1_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "table2_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:51.626965Z",
     "start_time": "2020-05-20T20:32:51.514043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bd2f37619a40a289287d01bde0da90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- LINE_ID: int\n",
      "|-- LINE_NUMBER: int\n",
      "|-- ORDER_ID: int\n",
      "|-- PRODUCT_ID: int\n",
      "|-- QUANTITY: int\n",
      "|-- UNIT_PRICE: decimal\n",
      "|-- DISCOUNT: decimal\n",
      "|-- SUPPLY_COST: decimal\n",
      "|-- TAX: decimal"
     ]
    }
   ],
   "source": [
    "table1_dyf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:54.690879Z",
     "start_time": "2020-05-20T20:32:54.575708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a9600280854b16a46228623ba4abef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- ORDER_ID: int\n",
      "|-- SITE_ID: double\n",
      "|-- ORDER_DATE: timestamp\n",
      "|-- SHIP_MODE: string"
     ]
    }
   ],
   "source": [
    "table2_dyf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:57.570646Z",
     "start_time": "2020-05-20T20:32:57.458504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c33aabafbfc4759b7961f45146b0119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table1_dyf.toDF().createOrReplaceTempView(\"sales_order_v\")\n",
    "table2_dyf.toDF().createOrReplaceTempView(\"sales_order_detail_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61ba30efeec4d89985023d4ce334dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| salesdb|            customer|      false|\n",
      "| salesdb|       customer_site|      false|\n",
      "| salesdb|             product|      false|\n",
      "| salesdb|    product_category|      false|\n",
      "| salesdb|         sales_order|      false|\n",
      "| salesdb|     sales_order_all|      false|\n",
      "| salesdb|  sales_order_detail|      false|\n",
      "| salesdb|            supplier|      false|\n",
      "|        |sales_order_detail_v|       true|\n",
      "|        |       sales_order_v|       true|\n",
      "+--------+--------------------+-----------+"
     ]
    }
   ],
   "source": [
    "# Notice the output column isTemporary to finf yout temp tables that are activie for this session\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:33:09.244279Z",
     "start_time": "2020-05-20T20:32:59.877787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcf313bab4941848e151a9bd68de77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LINE_ID: integer (nullable = true)\n",
      " |-- LINE_NUMBER: integer (nullable = true)\n",
      " |-- ORDER_ID: integer (nullable = true)\n",
      " |-- PRODUCT_ID: integer (nullable = true)\n",
      " |-- QUANTITY: integer (nullable = true)\n",
      " |-- UNIT_PRICE: decimal(38,10) (nullable = true)\n",
      " |-- DISCOUNT: decimal(38,10) (nullable = true)\n",
      " |-- SUPPLY_COST: decimal(38,10) (nullable = true)\n",
      " |-- TAX: decimal(38,10) (nullable = true)\n",
      " |-- site_id: double (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- ship_mode: string (nullable = true)\n",
      "\n",
      "98000"
     ]
    }
   ],
   "source": [
    "# Write the denormalized SALES_ORDER_FACT table\n",
    "df=spark.sql(\"SELECT a.*, b.site_id, b.order_date,b.ship_mode \\\n",
    "FROM sales_order_detail_v b, sales_order_v a \\\n",
    "WHERE a.order_id=b.order_id\")\n",
    "df.printSchema()\n",
    "print(df.count())\n",
    "df.coalesce(1).write.mode(\"OVERWRITE\").parquet(\"s3://glue-labs001-518010202968/data/sales_analytics/sales_order_fact/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the power of Spark SQL for this transformation instead of AWS Glue DynamicFrame transforms. This dataset is small so we also coalesced the number of partitions in the Spark dataframe to 1 to ensure only 1 file gets written to our output location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:33:13.463204Z",
     "start_time": "2020-05-20T20:33:12.735834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 116\n",
      "drwxr-xr-x 3 root       root       4096 Jun 18 01:23 brazil-npm-registry\n",
      "-rw-r--r-- 1 ec2-user   ec2-user    543 Jun 18 01:33 core-js-banners\n",
      "drwxr-xr-x 2 role-agent role-agent 4096 Jun 23 19:16 hsperfdata_role-agent\n",
      "drwxr-xr-x 3 role-agent role-agent 4096 Jun 23 19:16 jetty-localhost-9081-role-proxy-agent.war-_-any-3270408570273192286.dir\n",
      "drwxr-xr-x 3 root       root       4096 Jun 18 01:23 lib\n",
      "-rw-r--r-- 1 root       root          0 Jun 23 19:18 lifecycle-config-reconnect-dev-endpoint-daemon.lock\n",
      "-rw-r--r-- 1 root       root          0 Jun 23 19:18 lifecycle-config-v2-dev-endpoint-daemon.lock\n",
      "drwxr-xr-x 3 ec2-user   ec2-user   4096 Jun 18 01:33 npm-12080-1555bd66\n",
      "drwxr-xr-x 3 ec2-user   ec2-user   4096 Jun 18 01:35 npm-12474-c4af1476\n",
      "drwxr-xr-x 3 ec2-user   ec2-user   4096 Jun 18 01:35 npm-12503-7b4d78a8\n",
      "drwxr-xr-x 3 ec2-user   ec2-user   4096 Jun 18 01:36 npm-12716-137db1a0\n",
      "drwxr-xr-x 3 ec2-user   ec2-user   4096 Jun 18 01:36 npm-12756-5e28ef95\n",
      "drwxr-xr-x 3 ec2-user   ec2-user   4096 Jun 18 01:52 npm-16275-bc658ec1\n",
      "drwxr-xr-x 3 ec2-user   ec2-user   4096 Jun 18 01:52 npm-16304-8eeddd3a\n",
      "drwxr-xr-x 3 ec2-user   ec2-user   4096 Jun 18 01:52 npm-16315-2d16f9f5\n",
      "drwxr-xr-x 3 ec2-user   ec2-user   4096 Jun 18 01:52 npm-16326-6804f98b\n",
      "drwxr-xr-x 3 ec2-user   ec2-user   4096 Jun 18 01:34 v8-compile-cache-500\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:33 yarn--1592444009248-0.8214945508502562\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:33 yarn--1592444011640-0.9889156027517108\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:33 yarn--1592444019269-0.9468595048797366\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:35 yarn--1592444158127-0.8886679276837619\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:36 yarn--1592444168966-0.2859315411146428\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:36 yarn--1592444169997-0.2875851383841621\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:36 yarn--1592444171022-0.45750803422658737\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:36 yarn--1592444219006-0.8805486219706313\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:37 yarn--1592444220923-0.8949312593086283\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:37 yarn--1592444228764-0.98948225837253\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:37 yarn--1592444229789-0.6664395373191525\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:37 yarn--1592444230821-0.6456418042439396\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:52 yarn--1592445154533-0.03748272717505441\n",
      "drwxr-xr-x 2 ec2-user   ec2-user   4096 Jun 18 01:52 yarn--1592445156696-0.4982026441433651\n",
      "Requirement already up-to-date: scikit-learn in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from scikit-learn) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from scikit-learn) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# You can also run linux commands on your dev endpoint machine.\n",
    "%%sh\n",
    "ls -l /tmp\n",
    "pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the output data is in Amazon S3, let's crawl this dataset in AWS Glue and query this data using Amazon Athena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Crawl the Transformed Data\n",
    "\n",
    "- Navigate to the Glue console at Services -> Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **sales_analytics_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/sales_analytics/**\n",
    "    - Page: Choose an IAM role\n",
    "        - IAM Role: Choose an existing IAM role **glue-labs-GlueServiceRole**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database:  Click on ‘Add database’ and enter database name as **sales_analytics**.\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:35:32.257757Z",
     "start_time": "2020-05-20T20:35:31.387873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4454ab64608401b8250ea0942fcfc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---------------+--------------------+-----------+\n",
      "|       database|           tableName|isTemporary|\n",
      "+---------------+--------------------+-----------+\n",
      "|sales_analytics|        customer_dim|      false|\n",
      "|sales_analytics|         product_dim|      false|\n",
      "|               |sales_order_detail_v|       true|\n",
      "|               |       sales_order_v|       true|\n",
      "+---------------+--------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use sales_analytics\").show()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an AWS Glue Workflow\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "SO far in this exercise, you have created and tested your code. Once you developed the code and jobs you need, its time to orchestrate multiple jobs to deploy it and schedule to run on a periodic bases. This section helps you build the workflow. An AWS Glue workflow is an orchestration used to visualize and manage the relationship and execution of multiple AWS Glue triggers, jobs and crawlers. Let's now build an AWS Glue Workflow for the same. \n",
    "\n",
    "The 1st step is to create the AWS Glue Jobs. As the AWS Glue ETL code is already staged in our Amazon S3 bucket, we will simply call the AWS Glue APIs to create the AWS Glue Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:35:38.089828Z",
     "start_time": "2020-05-20T20:35:37.217679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Load_SALES_ORDER_FACT', 'ResponseMetadata': {'RequestId': 'cab5485d-21df-44ba-ba1b-103d95c5f6f8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 24 Jun 2020 14:59:31 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '32', 'connection': 'keep-alive', 'x-amzn-requestid': 'cab5485d-21df-44ba-ba1b-103d95c5f6f8'}, 'RetryAttempts': 0}}\n",
      "{'Name': 'Load_PRODUCT_DIM', 'ResponseMetadata': {'RequestId': '3342d9ef-c907-4612-8f23-9a3ab757e337', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 24 Jun 2020 14:59:31 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '27', 'connection': 'keep-alive', 'x-amzn-requestid': '3342d9ef-c907-4612-8f23-9a3ab757e337'}, 'RetryAttempts': 0}}\n",
      "{'Name': 'Load_CUSTOMER_DIM', 'ResponseMetadata': {'RequestId': '1c218cac-fd79-4d0a-addd-125b1addf1c4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 24 Jun 2020 14:59:31 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '28', 'connection': 'keep-alive', 'x-amzn-requestid': '1c218cac-fd79-4d0a-addd-125b1addf1c4'}, 'RetryAttempts': 0}}\n",
      "{'Name': 'Load_SUPPLIER_DIM', 'ResponseMetadata': {'RequestId': 'f1461fc2-6165-4f18-85ee-4cdc5964541b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 24 Jun 2020 14:59:31 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '28', 'connection': 'keep-alive', 'x-amzn-requestid': 'f1461fc2-6165-4f18-85ee-4cdc5964541b'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "%local\n",
    "\n",
    "import boto3\n",
    "\n",
    "acct_number=boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket='glue-labs-001-518010202968'\n",
    "\n",
    "# Create the AWS Glue Spark Jobs\n",
    "glue = boto3.client(\"glue\")\n",
    "\n",
    "for job_name in ['Load_SALES_ORDER_FACT', 'Load_PRODUCT_DIM', 'Load_CUSTOMER_DIM','Load_SUPPLIER_DIM']:\n",
    "    response=glue.create_job(Name=job_name,\n",
    "                         Role=f\"arn:aws:iam::{acct_number}:role/glue-labs-GlueServiceRole\",\n",
    "                         ExecutionProperty={'MaxConcurrentRuns': 1},\n",
    "                         Command={'Name': 'glueetl',\n",
    "                                  'ScriptLocation': f's3://{bucket}/scripts/{job_name}.py',\n",
    "                                  'PythonVersion': '3'},\n",
    "                         DefaultArguments={'--TempDir': f's3://{bucket}/temp',\n",
    "                                           '--enable-continuous-cloudwatch-log': 'true',\n",
    "                                           '--enable-glue-datacatalog': '',\n",
    "                                           '--enable-metrics': '',\n",
    "                                           '--enable-spark-ui': 'true',\n",
    "                                           '--spark-event-logs-path': f's3://{bucket}/spark_glue_etl_logs/{job_name}',\n",
    "                                           '--job-bookmark-option': 'job-bookmark-disable',\n",
    "                                           '--job-language': 'python',\n",
    "                                           '--S3_BUCKET': bucket },\n",
    "                         MaxRetries=0,\n",
    "                         Timeout=2880,\n",
    "                         MaxCapacity=3.0,\n",
    "                         GlueVersion='1.0',\n",
    "                         Tags={'Owner': 'Glue_Labs'}\n",
    "                        )\n",
    "    print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Workflow consists of 3 AWS Glue triggers:\n",
    "\n",
    "- The 1st OnDemand Trigger loads the Dimension tables.\n",
    "- The 2nd Conditional Trigger loads the Fact table.\n",
    "- The 3rd Conditional Trigger updated the table definitions in the Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:35:42.620317Z",
     "start_time": "2020-05-20T20:35:42.126763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Sales_Analytics_Workflow2', 'ResponseMetadata': {'RequestId': 'd134f5aa-b171-4825-9087-442daed7f209', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 24 Jun 2020 15:16:58 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '36', 'connection': 'keep-alive', 'x-amzn-requestid': 'd134f5aa-b171-4825-9087-442daed7f209'}, 'RetryAttempts': 0}}\n",
      "{'Name': '1_Load_Dimensions', 'ResponseMetadata': {'RequestId': '4769c63b-a5f4-4429-bbbb-e02564faf1ca', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 24 Jun 2020 15:16:58 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '28', 'connection': 'keep-alive', 'x-amzn-requestid': '4769c63b-a5f4-4429-bbbb-e02564faf1ca'}, 'RetryAttempts': 0}}\n",
      "{'Name': '2_Load_Facts', 'ResponseMetadata': {'RequestId': '8cd96fb7-2725-4ec6-8445-0f802ec6f110', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 24 Jun 2020 15:16:58 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '23', 'connection': 'keep-alive', 'x-amzn-requestid': '8cd96fb7-2725-4ec6-8445-0f802ec6f110'}, 'RetryAttempts': 0}}\n",
      "{'Name': '3_Update_Catalog', 'ResponseMetadata': {'RequestId': 'c0459ebd-3525-429d-a392-b0d98013ce95', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 24 Jun 2020 15:16:58 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '27', 'connection': 'keep-alive', 'x-amzn-requestid': 'c0459ebd-3525-429d-a392-b0d98013ce95'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "%local\n",
    "\n",
    "glue = boto3.client(\"glue\")\n",
    "\n",
    "# Create the AWS Glue Workflow\n",
    "response = glue.create_workflow(\n",
    "    Name='Sales_Analytics_Workflow2',\n",
    "    Description='Sales Analytics Workflow v1.0'\n",
    ")\n",
    "print (response)\n",
    "\n",
    "# 1. The Trigger to load the Dimensions table\n",
    "response = glue.create_trigger(\n",
    "    Name='1_Load_Dimensions',\n",
    "    WorkflowName='Sales_Analytics_Workflow',\n",
    "    Type='ON_DEMAND',\n",
    "    Actions=[{'JobName': 'Load_CUSTOMER_DIM',\n",
    "    'Arguments': {'--job-bookmark-option': 'job-bookmark-disable'},\n",
    "    'Timeout': 2880},\n",
    "   {'JobName': 'Load_PRODUCT_DIM',\n",
    "    'Arguments': {'--job-bookmark-option': 'job-bookmark-disable'},\n",
    "    'Timeout': 2880},\n",
    "   {'JobName': 'Load_SUPPLIER_DIM',\n",
    "    'Arguments': {'--job-bookmark-option': 'job-bookmark-disable'},\n",
    "    'Timeout': 2880}]\n",
    ")\n",
    "print (response)  \n",
    "\n",
    "# 2. The Trigger to load the Facts table\n",
    "response = glue.create_trigger(\n",
    "    Name='2_Load_Facts',\n",
    "    WorkflowName='Sales_Analytics_Workflow',\n",
    "    Type='CONDITIONAL',\n",
    "    StartOnCreation=True,\n",
    "    Actions=[{'JobName': 'Load_SALES_ORDER_FACT'}],\n",
    "    Predicate= {'Logical': 'AND',\n",
    "    'Conditions': [{'LogicalOperator': 'EQUALS',\n",
    "                  'JobName': 'Load_SUPPLIER_DIM',\n",
    "                   'State': 'SUCCEEDED'},\n",
    "                  {'LogicalOperator': 'EQUALS',\n",
    "                   'JobName': 'Load_PRODUCT_DIM',\n",
    "                   'State': 'SUCCEEDED'},\n",
    "                  {'LogicalOperator': 'EQUALS',\n",
    "                   'JobName': 'Load_CUSTOMER_DIM',\n",
    "                   'State': 'SUCCEEDED'}]\n",
    "               }\n",
    ")\n",
    "print (response)  \n",
    "\n",
    "# Finally, the Trigger for the Crawler\n",
    "response = glue.create_trigger(\n",
    "    Name='3_Update_Catalog',\n",
    "    WorkflowName='Sales_Analytics_Workflow',\n",
    "    Type='CONDITIONAL',\n",
    "    StartOnCreation=True,\n",
    "    Actions=[{'CrawlerName': 'sales_analytics_crawler'}],\n",
    "    Predicate= {'Logical': 'ANY',\n",
    "   'Conditions': [{'LogicalOperator': 'EQUALS',\n",
    "     'JobName': 'Load_SALES_ORDER_FACT',\n",
    "     'State': 'SUCCEEDED'}]}\n",
    ")\n",
    "print (response)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the AWS Glue Workflow created:\n",
    "    \n",
    "- Navigate to the Glue Console at Service -> Glue\n",
    "- From the left-hand panel menu, choose Workflows\n",
    "- Select the Workflow 'Sales_Analytics_Workflow'.\n",
    "\n",
    "Your workflow should look like this:\n",
    "\n",
    "![title](../resources/Glue_Workflow.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now run this workflow: \n",
    "\n",
    "- Select the workflow and click on 'Action - > Run' to launch the workflow\n",
    "- You can view the run details and visually track the progress of each acitvity in the workflow from the 'History' tab by selecting the workflow run and clicking on 'View Run Details'\n",
    "\n",
    "![title](../resources/View_Run_Details.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "\n",
    "In this notebook, we ran exercises to perform: \n",
    "\n",
    "1. A CSV to Parquet conversion and observed how easy it is to transform and write data to an Amazon S3 bucket using AWS Glue, partitioned by key fields.\n",
    "2. A more complex transformation - denormalizing of a 3NF OLTP schema, and we observed how easy it is to perform complex data transformations using the power of both AWS Glue DynamicFrames and Spark SQL.\n",
    "3. We built and executed an AWS Glue Workflow to orchestrate multiple AWS Glue Jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
